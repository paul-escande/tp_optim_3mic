{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmes de descente en optimisation différentiable sans contrainte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce TP utilisera les bibliothèques `numpy`, `matplotlib.pyplot`, `time` et `scipy` qui sont importées de cette façon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, nous allons aborder une méthode de débruitage d'image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Le bruit dans les images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une image $u$ de taille $M \\times N$ est une collection de pixels $MN$ organisés sous forme de tableau 2D de $M$ lignes et $N$ colonnes.\n",
    "\n",
    "Dans ce TP, nous manipulerons des images en niveau de gris.\n",
    "\n",
    "Le pixel de coordonnées $(i,j)$ pour $0 \\leq i \\leq M-1$ et $0 \\leq j \\leq N-1$ a une intensité $u[i,j] \\in [0,1]$. \n",
    "\n",
    "La librairy numpy permet de charger et manipuler les images. Les images peuvent être affichés en utilisant la fonction `plt.imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imread('image.jpg') # loading image, values are integers in [0,255]\n",
    "u = np.mean(np.double(u) / 255, axis = 2) # converting to grayscale and double precision in [0,1].\n",
    "plt.imshow(u, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De part la nature corpusculaire de la lumière et le fonctionnement des capteurs des appareils photos, les images collectées souffrent d'un bruit qui se modèlise par l'ajout d'une réalisation d'une variable aléatoire gaussienne sur chaque pixel de l'image. \n",
    "Il sera supposé que les variables aléatoires sont indépendantes et identiquement distribuées.\n",
    "Ainsi, au lieu d'oberver une image $u$, on observe sa version bruitée $v$ définie par\n",
    "$$\n",
    "    v[i,j] = u[i,j] + n[i,j], \\quad \\textrm{où } n[i,j] \\sim \\mathcal{N}(0,\\sigma^2).\n",
    "$$\n",
    "De manière compacte, cette équation s'écrit sous la forme\n",
    "$$\n",
    "    v = u + n, \\quad \\textrm{où } n \\sim \\mathcal{N}(0,\\sigma^2 \\textrm{Id}_{MN}).\n",
    "$$\n",
    "\n",
    "La variance $\\sigma^2$ modèlise l'importance du bruit dans l'image.\n",
    "\n",
    "La librairy `numpy` permet de générer des tableaux contenant des réalisations de varaibles aléatoire gaussienne. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Utiliser la fonction `np.random.randn` pour générer l'image $v$ associée à $u$. Faire varier $\\sigma$ et observer son effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En traitement d'image, une mesure courante des performances des algorithmes de débruitage est le `psnr` qui est définie par:\n",
    "$$\n",
    "    \\begin{split}\n",
    "    \\textrm{PSNR}(u,v) &= -10\\log_{10}\\left( \\frac{1}{MN} \\sum_{i,j} (u[i,j] - v[i,j])^2  \\right) \\\\\n",
    "                        &= -10\\log_{10}\\left( \\frac{1}{MN} \\| u - v \\|^2  \\right) \\\\\n",
    "    \\end{split}\n",
    "$$\n",
    "Le PSNR permet de mesurer sur une echelle logarithmique la distance entre les deux images. Plus il est élevé, plus l'image $v$ est proche de $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(u,v):\n",
    "    return -10 * np.log10(np.mean((u - v) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Observer l'influence de $\\sigma$ sur le PSNR(u,v)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Le débruitage comme un problème d'optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème de débruitage d'une image $v$ consiste à retrouver l'image $u$ sous-jacente à partir de $v$. \n",
    "Ceci est un **problème inverse**. Il existe plusieurs méthodes de débruitage des images. Dans ce TP nous allons estimer $u$ à partir de $v$ en minimisant la fonction\n",
    "$$\n",
    "    f(u) = \\frac{1}{2} \\| u - v \\|_2^2 + \\lambda R(u)\n",
    "$$\n",
    "où \n",
    "- $\\| u - v \\|_2^2 = \\sum_{i=1}^M  \\sum_{j=1}^N (u[i,j] - v[i,j])^2$ est le terme \"d'attache aux données\"\n",
    "- $R(u)$ est une fonction de régularisation qui est choisie pour être élevée pour les images contenant du bruit, et faible pour les images sans bruit.\n",
    "- $\\lambda >0$ est le paramètre de régularisation, qui permet d'opérer un compromis entre l'attache aux données et la régularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Choix de la régularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de régularisation $R$ qui sera utilisée dans ce TP, repose sur la notion de différences finies d'une image. Pour une image $u$, il est défini par\n",
    "$$\n",
    "    D u[i,j] = \\begin{pmatrix} D_1 u[i,j] \\\\ D_2 u[i,j] \\end{pmatrix}\n",
    "$$\n",
    "où $D_1$ et $D_2$ sont définies par différences finies:\n",
    "$$\n",
    "    D_1 u [i,j] = \\left\\{ \n",
    "        \\begin{split} \n",
    "            u[i+1,j] - u[i,j] & \\quad \\textrm{si } 0 \\leq i \\leq M-2 \\\\ \n",
    "            0 & \\quad \\quad \\textrm{si }  i = M-1 \\\\ \n",
    "        \\end{split} \n",
    "        \\right.\n",
    "$$\n",
    "$$\n",
    "    D_2 u [i,j] = \\left\\{ \n",
    "        \\begin{split} \n",
    "            u[i,j+1] - u[i,j] & \\quad \\textrm{si } 0 \\leq j \\leq N-2 \\\\ \n",
    "            0 & \\quad \\quad \\textrm{si }  j = N-1 \\\\ \n",
    "        \\end{split} \n",
    "        \\right.\n",
    "$$\n",
    "\n",
    "Attention, même si une image de taille $M \\times N$ est stockée sous forme de tableau 2d, elle peut également être vue comme un vecteur dans $\\mathbb{R}^{MN}$. Le but étant de pouvoir définir les applications linéaires agissant sur les images.  \n",
    "\n",
    "De ce point de vue, $D$ est une application linéaire de $\\mathbb{R}^{MN}$ dans $\\mathbb{R}^{2MN}$.\n",
    "\n",
    "\n",
    "On utilisera la notation $| Du |^2$ qui définit une image de taille $M \\times N$ par $| Du |^2[i,j] = D_1u[i,j]^2 + D_2u[i,j]^2$. On dit que $|Du|^2$ est la norme de $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Coder deux fonctions permettant de calculer $D_1 u$ et $D_2 u$.\n",
    "Il est possible d'utiliser des boucles pour ce faire. Néanmoins, python étant un language interprété, cette stratégie donnera lieu à des fonctions lentes.\n",
    "Une deuxième stratégie (environ 60 fois plus rapide) consiste à utiliser les mécanismes d'indexation des np.array pour implémenter ces opérations [(lien)](https://numpy.org/doc/stable/user/basics.indexing.html).\n",
    "\n",
    "En particulier, la fonction associée à $D_1$ peut être codée de la façon suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d1(u):\n",
    "    du=np.zeros(u.shape)\n",
    "    du[:-1,:] = u[1:,:] - u[:-1,:]\n",
    "    return du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier et comprendre le code de cette fonction puis coder la fonction associée à $D_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2(u):\n",
    "    ### à coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Calculer $|Du|$ et $|Dv|$ et les afficher. Commenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### à coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de régularisation $R$ qui sera utilisée dans ce TP est définie par :\n",
    "$$\n",
    "    R(u) = r(|D u|) = \\sum_{i,j} \\sqrt{\\epsilon^2 + |Du|[i,j]^2}\n",
    "$$ \n",
    "Le code suivant calcule la valeur de $R(v)$ où $v = u + n$ et $n \\sim \\mathcal{N}(0,\\sigma^2 \\textrm{Id})$ pour différentes valeurs de $\\sigma$. On observe que $\\sigma \\mapsto R(u + n)$ est croissante permettant ainsi de penaliser les images bruitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sigma = np.linspace(0,0.2,10) \n",
    "list_R = np.zeros_like(list_sigma)\n",
    "\n",
    "nbr_test = 10\n",
    "eps = 0.001 \n",
    "\n",
    "for i, sigma in enumerate(list_sigma):\n",
    "    for j in range(nbr_test):\n",
    "        v = u + sigma * np.random.randn(u.shape[0], u.shape[1])\n",
    "        dv = np.sqrt(d1(v)**2 + d2(v)**2) \n",
    "        list_R[i] += np.sum(np.sqrt(dv**2 + eps**2))\n",
    "list_R /= nbr_test\n",
    "\n",
    "plt.plot(list_sigma, list_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette régularisation est en réalité une version \"lissée\" d'une régularisation très utilisée en traitement d'image: la variation totale.\n",
    "L'utilisation de la variation totale en tant que régularisation permet de favoriser les images constantes par morceaux, une propriété que le bruit ne vérifie pas.\n",
    "Cependant, cette régularisation n'est pas différentiable, et sa minimisation telle quelle sort du cardre de ce cours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Etude de $f$ (si le temps le permet)\n",
    "\n",
    "Le graphe de la fonction $\\phi_\\epsilon$ peut être tracé pour différente valeurs de $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-0.5,0.5,1000)\n",
    "plt.plot(t,np.sqrt(t**2+1e-1**2))\n",
    "plt.plot(t,np.sqrt(t**2+1e-2**2))\n",
    "plt.plot(t,np.sqrt(t**2+1e-3**2))\n",
    "plt.plot(t,np.sqrt(t**2+1e-4**2))\n",
    "plt.legend(['$10^{-1}$', '$10^{-2}$', '$10^{-3}$', '$10^{-4}$'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Graphiquement, quelle est la limite de $\\phi_\\epsilon$ quand $\\epsilon \\to 0$ ?\n",
    "Montrer qu'elle est convexe sur $\\mathbb{R}$ et croissante sur $\\mathbb{R}^+$.\n",
    "\n",
    "**Todo:** En utilisant un exercice vu en TD, montrer que $f$ est une fonction convexe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Montrer que le gradient de $R$ est \n",
    "$$\n",
    "    \\nabla R(u) = D^T \\left( \\frac{D u}{\\sqrt{\\epsilon^2 + |Du|^2}} \\right) = D_1^T  \\left( \\frac{D_1 u}{{\\sqrt{\\epsilon^2 + |Du|^2}}}   \\right) + D_2^T  \\left( \\frac{D_2 u}{{\\sqrt{\\epsilon^2 + |Du|^2}}}   \\right) \n",
    "$$\n",
    "où la division s'entend terme à terme. C'est à dire que \n",
    "$$\n",
    "\\left( \\frac{D_1 u}{{\\sqrt{\\epsilon^2 + |Du|^2}}} \\right)[i,j] = \\frac{D_1 u [i,j]}{{\\sqrt{\\epsilon^2 + |Du|^2[i,j]}}}\n",
    "$$\n",
    "\n",
    "Déduire le gradient de $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'implémenter une descente de gradient pour minimiser $f$, il faut implémnter les adjoints de $D_1$ et $D_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adjoints de $D_1$ et $D_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les adjoints des applications linéaires $D_i : \\mathbb{R}^{MN} \\to \\mathbb{R}^{MN}$ peuvent être obtenus en observant que\n",
    "$$\n",
    "    \\langle D_i u, v \\rangle_{\\mathbb{R}^{MN}} = \\langle u, D_i^T v \\rangle_{\\mathbb{R}^{MN}}, \\quad \\forall u,v \\in \\mathbb{R}^{MN}.\n",
    "$$\n",
    "Ceci donne :\n",
    "$$\n",
    "    D_1^T u [i,j] = \\left\\{ \n",
    "        \\begin{split} \n",
    "            -u[0,j] & \\quad \\textrm{si } 0 \\leq i = 0 \\\\ \n",
    "            u[i-1, j] - u[i,j] & \\quad \\quad \\textrm{si } 1 \\leq i \\leq M-2 \\\\ \n",
    "            u[M-2,j] & \\quad \\quad \\textrm{si }  i = M-1 \\\\ \n",
    "        \\end{split} \n",
    "        \\right.\n",
    "$$\n",
    "$$\n",
    "    D_2^T u [i,j] = \\left\\{ \n",
    "        \\begin{split} \n",
    "            -u[i,0] & \\quad \\textrm{si } 0 \\leq j = 0 \\\\ \n",
    "            u[i, j-1] - u[i,j] & \\quad \\quad \\textrm{si } 1 \\leq j \\leq N-2 \\\\ \n",
    "            u[i,N-2] & \\quad \\quad \\textrm{si }  j = N-1 \\\\ \n",
    "        \\end{split} \n",
    "        \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Coder deux fonctions permettant de calculer $D_1^T u$ et $D_2^T u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d1T(u):\n",
    "### à coder\n",
    "\n",
    "def d2T(u):\n",
    "### à coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test de l'exactitude de l'implémentation:** Pour rappel, une matrice $A : \\R^m \\to \\R^n$ et son adjoint $A^T$ sont reliées par la relation\n",
    "$$ \n",
    "    \\langle A u, v \\rangle_{\\R^n} = \\langle u, A^T v\\rangle_{\\R^m}, \\quad \\forall u \\in \\R^m, \\forall v \\in \\R^n.\n",
    "$$\n",
    "Afin de vérifier que $D_1$ et $D_1^T$ sont bien adjoints il est alors possible de générer aléatoirement des vecteurs $u,v \\in \\R^{MN}$ et de tester que \n",
    "$$ \n",
    "    \\langle D_1 u, v \\rangle - \\langle u, D_1^T v\\rangle\n",
    "$$\n",
    "vaut 0. En pratique, l'arithmétique finie ne permet pas d'atteindre 0 et il seulement possible d'atteindre des valeurs proches de la pécision machine.\n",
    "Le code suivant réalise les tests pour $D_1, D_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(u.shape[0],u.shape[1]) \n",
    "y = np.random.rand(u.shape[0],u.shape[1]) \n",
    "\n",
    "test1 = np.sum(d1(x)*y) - np.sum(x*d1T(y))\n",
    "test2 = np.sum(d2(x)*y) - np.sum(x*d2T(y))\n",
    "\n",
    "print('Adjoint test d1, d1T : %e, should be of the order 1e-14' % (test1)) \n",
    "print('Adjoint test d2, d2T : %e, should be of the order 1e-14' % (test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Constante de Lipshitz du gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de montrer que la constante de Lipschitz de $\\nabla R$ est $\\frac{8}{\\epsilon}$.\n",
    "\n",
    "**Todo:** En déduire un majorant $L$ de la constante de Lipschitz de $\\nabla f$ et interpréter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémenter l'algorithme de descente de gradient à pas fixe\n",
    "\n",
    "$$ x_{k+1} = x_k - s \\nabla f(x_k) $$\n",
    "\n",
    "* Choisir $x_0 = v$.\n",
    "* Le critère d'arrêt sera sur la norme du gradient:\n",
    "$$\n",
    "    \\frac{\\| \\nabla f(x_k) \\|}{\\| \\nabla f(v) \\|}  < \\eta\n",
    "$$\n",
    "et un nombre d'itérations maximal de 20000.\n",
    "Calculer à chaque itération la valeur de $f(x_k)$ et les stocker dans une liste.\n",
    "* Calculer le temps nécessaire pour la convergence de l'algorithme avec la fonction `time.time()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.05\n",
    "v = u + sigma * np.random.randn(u.shape[0], u.shape[1])\n",
    "\n",
    "nit = 20000\n",
    "epsilon = 0.001\n",
    "l = 0.06\n",
    "x = v\n",
    "L = 1 + l* 8 / epsilon\n",
    "step = 2/L\n",
    "\n",
    "eta = 1e-2\n",
    "\n",
    "listcf = [] \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "### descente à coder\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Tester la méhode pour un bruit de niveau $\\sigma = 0.05$, choisir $\\epsilon = 0.001$,  $\\eta = 10^{-2}$ et $\\lambda = 0.06$. \n",
    "\n",
    "Le code suivant affiche le psnr de la solution, la fonction cout en echelle semilogy, le nombre d'itérations necessaire, le temps de calcul et le temps de calul par itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listcf_grad = np.array(listcf)\n",
    "plt.figure()\n",
    "plt.semilogy(listcf_grad)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(x, cmap='gray')\n",
    "print('Psnr denoised: %.2fdB' % (psnr(u, x)))\n",
    "print('Number of iterations: %d' % (i))\n",
    "print('Elapsed time: %.2f s' % (end - start))\n",
    "print('Time spent per iteration: %.2fms' % (1000*(end - start)/i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Changer les valeurs de $\\sigma$, $\\lambda$ et $\\epsilon$ pour oberver leurs effets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Méthode de quasi-Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons implémenter une méthode de quasi-Newton pour minimiser $f$. Les itérations sont de la forme :\n",
    "$$ x_{k+1} = x_k - H_k^{-1} \\nabla f(x_k) $$\n",
    "où les $H_k$ sont des approximations des matrices hessiennes $H[f](x_k)$.\n",
    "Dans ce TP nous allons utiliser les matrices :\n",
    "$$\n",
    "    H_k = \\textrm{Id} + \\lambda D^* W_k D, \\quad \\text{avec } W_k = \\textrm{diag}\\left(\\frac{1}{\\sqrt{\\epsilon^2 + |Du|^2}}\\right)\n",
    "$$\n",
    "c'est à dire qu'un produit matrice-vecteur avec le vecteur $d$ donne\n",
    "$$\n",
    "    H_k d = d + \\lambda \\left( D_1^T \\left( \\frac{D_1 d}{\\sqrt{\\epsilon^2 + |Du|^2}} \\right) + D_2^T \\left( \\frac{D_2 d}{\\sqrt{\\epsilon^2 + |Du|^2}} \\right) \\right)\n",
    "$$\n",
    "où la division s'entend terme à terme.\n",
    "\n",
    "La difficulté de la mise en oeuvre d'une méthode de quasi-Newton est la résolution du système linéaire. Pour une image de taille $256 \\times 256$ stocker la matrice $H_k$ en mémoire coûte 32GB et inverser une telle matrice exactement est impossible. Heuresement, la méthode de quasi-Newton nécessite seuelement une bonne approximation de la solution du système linéaire pour foncitonner. Une telle approximation peut être calculée en utilisant une méthode itérative (gradient conjugué) qui nécessite uniquement des fonctions capables de calculer les produits matrice-vecteur $H_k d$ pour tout $d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Produit matrice-vecteur avec $H_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Implémenter une fonction `mvp_H` implémentant le produit matrice vecteur $H d$ où $H$ est l'approximation de $H[f](u)$ définie précédemment. Cette fonction prend en entrée :\n",
    "- `normGrad` l'image de taille $M \\times N$ contenant $\\sqrt{\\epsilon^2 + |Du|^2}$\n",
    "- `d` une image de taille $M \\times N$\n",
    "- `l` la valeur de $\\lambda$, le paramètre de régularisation\n",
    "- `epsilon` la valeur de $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### à coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Résolution du système linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le système linéaire $H d = v$ peut maintenant être résolu en utilisant une méthode de gradient conjugué (non-abordée dans ce cours).\n",
    "Cet algorithme est implémenté dans la librairy `scipy`. Vous pouvez utiliser la fonction `newton_direction` suivante afin d'obtenir la direction $d$ définie par $H d = b$.\n",
    "Les arguments de cette fonction sont:\n",
    "- `normGrad` l'image de taille $M \\times N$ contenant $\\sqrt{\\epsilon^2 + |Du|^2}$ où $u$\n",
    "- `b` une image de taille $M \\times N$\n",
    "- `l` la valeur de $\\lambda$, le paramètre de régularisation\n",
    "- `epsilon` la valeur de $\\epsilon$\n",
    "- `rtol` la tolérance de résolution du système linéaire. `rtol = 1e-1` donne de bons résultats dans ce cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_direction(normGrad,l, b, epsilon, rtol=1e-2):\n",
    "    mvp = lambda h : mvh_H(normGrad, h.reshape(normGrad.shape[0],normGrad.shape[1]), l, epsilon).reshape(-1)\n",
    "    H = scipy.sparse.linalg.LinearOperator(shape = (normGrad.shape[0]*normGrad.shape[1],normGrad.shape[0]*normGrad.shape[1]), matvec = mvp, rmatvec = mvp)\n",
    "    d,info = scipy.sparse.linalg.cg(H,b = b.reshape(-1), x0 = b.reshape(-1), rtol = 1e-1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implémentation de la descente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Implémenter la descente de quasi-Newton.\n",
    "Comme pour la descente de gradient: \n",
    "* Choisir $x_0 = v$.\n",
    "* Le critère d'arrêt sera sur la norme du gradient:\n",
    "$$\n",
    "    \\frac{\\| \\nabla f(x_k) \\|}{\\| \\nabla f(v) \\|}  < \\eta\n",
    "$$\n",
    "et un nombre d'itérations maximal de 100.\n",
    "* Calculer à chaque itération la valeur de $f(x_k)$ et les stocker dans une liste.\n",
    "* Calculer le temps nécessaire pour la convergence de l'algoithme avec la fonction `time.time()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nit = 100\n",
    "epsilon = 0.001\n",
    "l = 0.06\n",
    "x = v\n",
    "\n",
    "listcf = [] \n",
    "list_err = []\n",
    "\n",
    "### descente à coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Tester la méhode pour un bruit de niveau $\\sigma = 0.05$, choisir $\\epsilon = 0.001$,  $\\eta = 10^{-2}$ et $\\lambda = 0.06$. \n",
    "\n",
    "Le code suivannt affiche le psnr la solution, la fonction cout en echelle semilogy, le nombre d'itérations necessaire, le temps de calcul et le temps de calul par itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listcf_newton = np.array(listcf)\n",
    "plt.figure()\n",
    "plt.semilogy(listcf)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(x, cmap='gray')\n",
    "print('Psnr denoised: %.2fdB' % (psnr(u, x)))\n",
    "print('Number of iterations: %d' % (i))\n",
    "print('Elapsed time: %.2f s' % (end - start))\n",
    "print('Time spent per iteration: %.2fms' % (1000*(end - start)/i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Changer les valeurs de $\\sigma$, $\\lambda$ et $\\epsilon$ pour oberver leurs effets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Comparaison avec la descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo** : comparer les performances avec la descente de gradient. On pourra en particulier regarder les performances des deux algorithmes pour $\\epsilon = 10^{-2},10^{-3},10^{-4}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pat (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
